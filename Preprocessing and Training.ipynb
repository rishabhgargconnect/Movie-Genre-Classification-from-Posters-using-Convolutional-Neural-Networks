{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the required libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import cv2\n",
    "import requests  \n",
    "import re  \n",
    "from urllib.request import urlretrieve \n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imdbId</th>\n",
       "      <th>Imdb Link</th>\n",
       "      <th>Title</th>\n",
       "      <th>IMDB Score</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Poster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>114709</td>\n",
       "      <td>http://www.imdb.com/title/tt114709</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>8.3</td>\n",
       "      <td>Animation|Adventure|Comedy</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>113497</td>\n",
       "      <td>http://www.imdb.com/title/tt113497</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>6.9</td>\n",
       "      <td>Action|Adventure|Family</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   imdbId                           Imdb Link             Title  IMDB Score  \\\n",
       "0  114709  http://www.imdb.com/title/tt114709  Toy Story (1995)         8.3   \n",
       "1  113497  http://www.imdb.com/title/tt113497    Jumanji (1995)         6.9   \n",
       "\n",
       "                        Genre  \\\n",
       "0  Animation|Adventure|Comedy   \n",
       "1     Action|Adventure|Family   \n",
       "\n",
       "                                              Poster  \n",
       "0  https://images-na.ssl-images-amazon.com/images...  \n",
       "1  https://images-na.ssl-images-amazon.com/images...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read IMDB dataset information from csv file \n",
    "movie = pd.read_csv(\"./MovieGenre.csv\", encoding = \"ISO-8859-1\")\n",
    "movie.dropna(inplace=True)\n",
    "\n",
    "# save required columns in a dataframe\n",
    "df_poster = movie[['imdbId','Poster']]\n",
    "\n",
    "movie.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables\n",
    "\n",
    "img_loc = \"original_1000_images/\"\n",
    "SIZE=(150, 101, 3)\n",
    "num_labels = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'word2idx': {'Animation': 0, 'Adventure': 1, 'Comedy': 2, 'Action': 3, 'Family': 4, 'Romance': 5, 'Drama': 6, 'Crime': 7, 'Thriller': 8, 'Fantasy': 9, 'Horror': 10, 'Biography': 11, 'History': 12, 'Mystery': 13, 'Sci-Fi': 14, 'War': 15, 'Sport': 16, 'Music': 17, 'Documentary': 18, 'Musical': 19, 'Western': 20, 'Short': 21, 'Film-Noir': 22, 'Talk-Show': 23, 'News': 24, 'Adult': 25, 'Reality-TV': 26, 'Game-Show': 27}, 'idx2word': ['Animation', 'Adventure', 'Comedy', 'Action', 'Family', 'Romance', 'Drama', 'Crime', 'Thriller', 'Fantasy', 'Horror', 'Biography', 'History', 'Mystery', 'Sci-Fi', 'War', 'Sport', 'Music', 'Documentary', 'Musical', 'Western', 'Short', 'Film-Noir', 'Talk-Show', 'News', 'Adult', 'Reality-TV', 'Game-Show']}\n"
     ]
    }
   ],
   "source": [
    "df_movietotal = movie\n",
    "\n",
    "# list of all the genres in original dataset\n",
    "\n",
    "label_dict = {\"word2idx\": {}, \"idx2word\": []}\n",
    "idx = 0\n",
    "genre_per_movie = df_movietotal[\"Genre\"].apply(lambda x: str(x).split(\"|\"))\n",
    "for l in [g for d in genre_per_movie for g in d]:\n",
    "    if l in label_dict[\"idx2word\"] or l=='':\n",
    "        pass\n",
    "    else:\n",
    "        label_dict[\"idx2word\"].append(l)\n",
    "        label_dict[\"word2idx\"][l] = idx\n",
    "        idx += 1\n",
    "n_classes = len(label_dict[\"idx2word\"])\n",
    "print(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove poster genres with less data (preserve most common genres only - final popular 10 genres)\n",
    "\n",
    "df_movietotal['Genre'].replace('Game-Show', '', inplace=True, regex=True)\n",
    "df_movietotal['Genre'].replace('Musical', '', inplace=True, regex=True)\n",
    "df_movietotal['Genre'].replace('Reality-TV', '', inplace=True, regex=True)\n",
    "df_movietotal['Genre'].replace('Adult', '', inplace=True, regex=True)\n",
    "df_movietotal['Genre'].replace('News', '', inplace=True, regex=True)\n",
    "df_movietotal['Genre'].replace('Talk-Show', '', inplace=True, regex=True)\n",
    "df_movietotal['Genre'].replace('Film-Noir', '', inplace=True, regex=True)\n",
    "df_movietotal['Genre'].replace('Short', '', inplace=True, regex=True)\n",
    "df_movietotal['Genre'].replace('Western', '', inplace=True, regex=True)\n",
    "df_movietotal['Genre'].replace('Sport', '', inplace=True, regex=True)\n",
    "df_movietotal['Genre'].replace('Documentary', '', inplace=True, regex=True)\n",
    "df_movietotal['Genre'].replace('Music', '', inplace=True, regex=True)\n",
    "df_movietotal['Genre'].replace('War', '', inplace=True, regex=True)\n",
    "df_movietotal['Genre'].replace('History', '', inplace=True, regex=True)\n",
    "df_movietotal['Genre'].replace('Biography', '', inplace=True, regex=True)\n",
    "df_movietotal['Genre'].replace('Crime', '', inplace=True, regex=True)\n",
    "df_movietotal['Genre'].replace('Fantasy', '', inplace=True, regex=True)\n",
    "df_movietotal['Genre'].replace('Family', '', inplace=True, regex=True)\n",
    "df_movietotal['Genre'].replace('', np.nan, inplace=True)\n",
    "\n",
    "df_movietotal.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movietotal_copy = df_movietotal.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'word2idx': {'Animation': 0, 'Adventure': 1, 'Comedy': 2, 'Action': 3, 'Romance': 4, 'Drama': 5, 'Thriller': 6, 'Horror': 7, 'Mystery': 8, 'Sci-Fi': 9}, 'idx2word': ['Animation', 'Adventure', 'Comedy', 'Action', 'Romance', 'Drama', 'Thriller', 'Horror', 'Mystery', 'Sci-Fi']}\n"
     ]
    }
   ],
   "source": [
    "# dictionary to store final genre class names and their corresponding ids\n",
    "\n",
    "label_dict = {\"word2idx\": {}, \"idx2word\": []}\n",
    "idx = 0\n",
    "genre_per_movie = df_movietotal_copy[\"Genre\"].apply(lambda x: str(x).split(\"|\"))\n",
    "for l in [g for d in genre_per_movie for g in d]:\n",
    "    if l in label_dict[\"idx2word\"] or l=='':\n",
    "        pass\n",
    "    else:\n",
    "        label_dict[\"idx2word\"].append(l)\n",
    "        label_dict[\"word2idx\"][l] = idx\n",
    "        idx += 1\n",
    "n_classes = len(label_dict[\"idx2word\"])\n",
    "print(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate number of dataset images for each genre class \n",
    "\n",
    "def genre_count(df, label_dict):\n",
    "    max_genre = 0\n",
    "    for label in label_dict[\"idx2word\"]:\n",
    "        occurrences = len((df[df['Genre'].str.contains(label)]))\n",
    "        print(label, occurrences)\n",
    "        if occurrences > max_genre:\n",
    "            max_genre = occurrences\n",
    "    return max_genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Animation 1704\n",
      "Adventure 3806\n",
      "Comedy 12562\n",
      "Action 5307\n",
      "Romance 6188\n",
      "Drama 19850\n",
      "Thriller 4797\n",
      "Horror 3981\n",
      "Mystery 2387\n",
      "Sci-Fi 2002\n",
      "max count of a label :  19850\n"
     ]
    }
   ],
   "source": [
    "# list of all genres and their corresponding dataset image count\n",
    "\n",
    "max_genre = genre_count(df_movietotal_copy, label_dict)\n",
    "print(\"max count of a label : \",max_genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movietotal_copy = df_movietotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oversampling almost doubles the dataset, not enough processing power to handle that\n",
    "\n",
    "# # IMBALANCE: OVERSAMPLING SOLUTION\n",
    "# df_movietotal_copy_2 = df_movietotal_copy\n",
    "# df_movietotal_copy_2 = df_movietotal_copy_2[~df_movietotal_copy_2[\"Genre\"].str.contains(\"Comedy\")]\n",
    "# df_movietotal_copy_2 = df_movietotal_copy_2[~df_movietotal_copy_2[\"Genre\"].str.contains(\"Drama\")]\n",
    "   \n",
    "# for label in label_dict[\"idx2word\"]:\n",
    "#     if label not in [\"Drama\", \"Comedy\"]:\n",
    "#         len_genre = len(df_movietotal_copy[df_movietotal_copy['Genre'].str.contains(label)])\n",
    "#         df_genre = df_movietotal_copy_2[df_movietotal_copy_2['Genre'].str.contains(label)]\n",
    "#         #df_genre['genres'] = [label+\"|\" for i in range (0, len(df_genre))]    \n",
    "#         if (len_genre) > 0:\n",
    "#             if len_genre > 5000:\n",
    "#                 param = 1000\n",
    "#             elif len_genre > 3000:\n",
    "#                 param = 3000\n",
    "#             elif len_genre > 1000:\n",
    "#                 param = 6000\n",
    "#             else:\n",
    "#                 param = 6000\n",
    "#             df_class_over = df_genre.sample(param, replace=True)\n",
    "#             df_movietotal_copy = pd.concat([df_movietotal_copy, df_class_over], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# undersampling of dataset : approximately 1000 images per genre selected (one image contains multiple genres)\n",
    "\n",
    "df_1000_new = pd.DataFrame()\n",
    "i=0\n",
    "for label in label_dict[\"idx2word\"]:\n",
    "    df_genre = df_movietotal_copy[df_movietotal_copy['Genre'].str.contains(label)]\n",
    "    if(i%2==0):\n",
    "        df_class_over = df_genre.iloc[0:1000]\n",
    "    else:\n",
    "        df_class_over = df_genre.iloc[-1000:]\n",
    "    df_1000_new = pd.concat([df_1000_new, df_class_over], axis=0)\n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 6)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1000_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random under-sampled Image count :\n",
      "Animation 1459\n",
      "Adventure 2091\n",
      "Comedy 3019\n",
      "Action 2478\n",
      "Romance 1720\n",
      "Drama 4067\n",
      "Thriller 2175\n",
      "Horror 2024\n",
      "Mystery 1528\n",
      "Sci-Fi 1566\n",
      "4067\n"
     ]
    }
   ],
   "source": [
    "print('Random under-sampled Image count :')\n",
    "print(genre_count(df_1000_new, label_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movietotal = df_1000_new\n",
    "#df_movietotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download posters using the poster links given in the dataset\n",
    "\n",
    "not_found = []\n",
    "http_404 = []\n",
    "for index, row in df_movietotal.iterrows():\n",
    "    url = row['Poster']\n",
    "    #print(index,url)\n",
    "    if \"https://images-na.ssl-images-amazon.com/\" in str(url):\n",
    "        id = row['imdbId']\n",
    "        jpgname = img_loc+str(id)+'.jpg'\n",
    "        try:\n",
    "            urlretrieve(url, jpgname)\n",
    "        except:\n",
    "            http_404.append(index)\n",
    "            pass\n",
    "        \n",
    "    else:\n",
    "        not_found.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(http_404)\n",
    "# len(not_found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store path of each poster and get ids from their path name\n",
    "\n",
    "image_glob = glob.glob(img_loc + \"*.jpg\")\n",
    "img_dict = {}\n",
    "\n",
    "def get_id(filename):\n",
    "    index_s = filename.rfind(\"/\") + 1\n",
    "    index_f = filename.rfind(\".jpg\")\n",
    "    return filename[index_s:index_f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# APPROACH 1 : NORMAL POSTERS - run this block of code for approach 1\n",
    "\n",
    "for fn in image_glob:\n",
    "    try:\n",
    "        img_dict[get_id(fn)] = cv2.imread(fn)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# APPROACH 2 : CENTER EXTRACTED POSTERS - run this block of code for approach 2\n",
    "\n",
    "for fn in image_glob:\n",
    "    try:\n",
    "        img_read = cv2.imread(fn)\n",
    "        img_final = img_read[int(SIZE[0]/8):SIZE[0]-int(SIZE[0]/8),int(SIZE[1]/8):SIZE[1]-int(SIZE[1]/8)]\n",
    "        cv2.imwrite(center_loc+get_id(fn)+\".jpg\", img_final)\n",
    "        img_dict[get_id(fn)] = img_final\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# APPROACH 3 : OBJECT BOUNDED POSTERS - run this block of code for approach 3\n",
    "\n",
    "# standard object detection used to extract the portion of poster with objects\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import cvlib as cv\n",
    "from cvlib.object_detection import draw_bbox\n",
    "import numpy as np\n",
    "\n",
    "def object_detector(im,SIZE=(150, 101, 3)):\n",
    "    # all bounding boxes coordinates extracted from posters\n",
    "    bbox, label, conf = cv.detect_common_objects(im)\n",
    "    # if no object found, return the original poster\n",
    "    if(bbox==[]):\n",
    "        return im\n",
    "    \n",
    "    minimum = np.min(bbox,axis=0)\n",
    "    maximum = np.max(bbox,axis=0)\n",
    "    \n",
    "    # calculate minimum x,y and maximum x,y from all the bounding box coordinates\n",
    "    f_x_min = np.min([minimum[0],minimum[2]])\n",
    "    f_y_min = np.min([minimum[1],minimum[3]])\n",
    "    if(f_x_min<0):\n",
    "        f_x_min=0\n",
    "    if(f_y_min<0):\n",
    "        f_y_min=0\n",
    "\n",
    "    f_x_max = np.max([maximum[0],maximum[2]])\n",
    "    f_y_max = np.max([maximum[1],maximum[3]])\n",
    "    \n",
    "    # using these new maximum and minimum x and y values, extract objects from poster\n",
    "    #(crop that specific portion from the poster)\n",
    "    im=im[f_y_min:f_y_max,f_x_min:f_x_max]\n",
    "    return im\n",
    "\n",
    "\n",
    "for fn in image_glob:\n",
    "    try:\n",
    "        img_read = cv2.imread(fn)\n",
    "        img_final = object_detector(img_read,SIZE)\n",
    "        # if no object detected, use border cropped posters\n",
    "        if len(img_final)==0:\n",
    "            img_final=img_read[int(SIZE[0]/8):SIZE[0]-int(SIZE[0]/8),int(SIZE[1]/8):SIZE[1]-int(SIZE[1]/8)]\n",
    "        cv2.imwrite(obj_loc+get_id(fn)+\".jpg\", img_final)\n",
    "        img_dict[get_id(fn)] = img_final\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image preprocessing - resizing and normalization\n",
    "import skimage\n",
    "def preprocess(img, size=(150, 101, 3)):\n",
    "    img = skimage.transform.resize(img, size)\n",
    "    img = img.astype(np.float32)\n",
    "    img = (img / 127.5) - 1.\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the images for convolutional neural networks - generate label array for each poster using one hot encoding\n",
    "# and preprocess the images \n",
    "\n",
    "def prepare_data(data, img_dict, label_dict, size=(150, 101, 3)):\n",
    "    print(\"Generation dataset...\")\n",
    "    dataset = []\n",
    "    y = []\n",
    "    ids = []\n",
    "    n_samples = len(img_dict)\n",
    "    print(\"got {} posters\".format(n_samples))\n",
    "    for k in img_dict:\n",
    "        #print(data['imdbId'].values)\n",
    "        if int(k) in data[\"imdbId\"].values:\n",
    "            G = data[data[\"imdbId\"] == int(k)][\"Genre\"].values\n",
    "            #print(G)\n",
    "            for g in G:\n",
    "                g = g.split(\"|\")\n",
    "                \n",
    "                img = preprocess(img_dict[k], size)\n",
    "                if img.shape != (150, 101, 3):\n",
    "                    continue\n",
    "                l = np.sum([np.eye(n_classes, dtype=\"uint8\")[label_dict[\"word2idx\"][s]] for s in g if s !=''], axis=0)\n",
    "                y.append(l)\n",
    "                dataset.append(img)\n",
    "                ids.append(k)\n",
    "    print(\"DONE\")\n",
    "    print(len(dataset))\n",
    "    return dataset, y, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data, labels and ids generated for entire dataset\n",
    "\n",
    "df_movietotal = df_movietotal[['Genre', 'imdbId', 'Title']]\n",
    "dataset, y, ids =  prepare_data(df_movietotal, img_dict, label_dict, size=SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data and labels for all 3 approaches saved - used in testing \n",
    "\n",
    "import pickle\n",
    "dbfile_1=open('datasetFile_object','ab')\n",
    "dbfile_2=open('yFile_object','ab')\n",
    "pickle.dump(y,dbfile_2)\n",
    "pickle.dump(dataset,dbfile_1)\n",
    "dbfile_1.close()\n",
    "dbfile_2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training \n",
    "\n",
    "Training was done using High Performance Research Computing resources. The code is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "data_train, data_test, y_train, y_test = train_test_split(dataset, y, test_size=0.10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional Neural Network\n",
    "\n",
    "# Sequential model used\n",
    "model = Sequential()\n",
    "\n",
    "# CONV => CONV => MAX POOL\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu',\n",
    "                 input_shape=(SIZE[0], SIZE[1], 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# CONV => CONV => MAX POOL\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# FLATTEN => DENSE => DENSE\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_labels, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary crossentropy used for loss calculation\n",
    "# Stochastic Gradient Descdent used for optimization\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning performed to determine optimal epochs and batch size for each approach\n",
    "\n",
    "epochs = [50,75,100]\n",
    "batches = [10,20]\n",
    "\n",
    "for e in epochs:\n",
    "    for b in batches:\n",
    "        start = datetime.datetime.now()\n",
    "        model.fit(np.array(data_train), np.array(y_train), batch_size=b, epochs=e, validation_split=0.1)\n",
    "        end = datetime.datetime.now()\n",
    "        \n",
    "        # each model is saved - used later for testing\n",
    "        model.save(\"model_\"+str(e)+\"_\"+str(b)+\".model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
