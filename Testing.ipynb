{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all required libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import load_model\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables\n",
    "\n",
    "model_original_path = \"modelReport/\"\n",
    "model_center_path = \"modelReport_center/\"\n",
    "model_object_path = \"modelReport_object/\"\n",
    "threshold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert probabilities to 0 and 1 predictions using threshold of 0.5\n",
    "\n",
    "def binary_prediction(pred):\n",
    "    value = 0\n",
    "    count=0\n",
    "    for p in range(0,len(pred)):\n",
    "        for i in range(0,len(pred[p])):\n",
    "            if(pred[p][i]>=threshold):\n",
    "                pred[p][i]=1\n",
    "            else:\n",
    "                pred[p][i]=0\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric 3 : Percentage correct predictions\n",
    "\n",
    "percentage of genres correctly predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_predictions_top(y_test, pred):\n",
    "    count=0\n",
    "    sum_count = 0\n",
    "    total_correct = 0\n",
    "    for p in range(0,len(pred)):\n",
    "        count = len( np.where(y_test[p])[0] )\n",
    "        sum_count+=count\n",
    "        sorted_pred = np.sort(pred[p])\n",
    "        top_pred = sorted_pred[-count:]\n",
    "        #print(top_pred)\n",
    "        for t in top_pred:\n",
    "            ind = np.where(pred[p]==t)\n",
    "            if(y_test[p][ind[0][0]]==1):\n",
    "                total_correct += 1\n",
    "                \n",
    "    return total_correct/sum_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy : Metric 1 and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric 1 : using confusion matrix\n",
    "\n",
    "def accuracy_score_ones_zeros(y_test, pred):\n",
    "    value = 0\n",
    "    count=0\n",
    "    for p in range(0,len(pred)):\n",
    "        for i in range(0,len(pred[p])):\n",
    "            if((pred[p][i]>=threshold and y_test[p][i]==1) or (pred[p][i]<threshold and y_test[p][i]==0)):\n",
    "                count+=1\n",
    "    return count/(10*len(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric 2 : using partial correctness\n",
    "\n",
    "def accuracy_score_weighted(y_test,pred_b):\n",
    "    y_true = np.array(y_test)\n",
    "    y_pred = np.array(pred_b)\n",
    "    acc_list=[]\n",
    "    for i in range(y_true.shape[0]):\n",
    "        #print(i)\n",
    "        set_true = set( np.where(y_true[i])[0] )\n",
    "        set_pred = set( np.where(y_pred[i])[0] )\n",
    "        tmp_a = None\n",
    "        if len(set_true) == 0 and len(set_pred) == 0:\n",
    "            tmp_a = 1\n",
    "        else:\n",
    "            tmp_a = len(set_true.intersection(set_pred))/float( len(set_true.union(set_pred)))\n",
    "        acc_list.append(tmp_a)\n",
    "    return np.mean(acc_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision : Metric 1 and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric 1 : using confusion matrix\n",
    "\n",
    "def precision_score_ones_zeros(y_test, pred):\n",
    "    value = 0\n",
    "    TP=0\n",
    "    FP=0\n",
    "    for p in range(0,len(pred)):\n",
    "        for i in range(0,len(pred[p])):\n",
    "            if(pred[p][i]>=threshold and y_test[p][i]==1):\n",
    "                TP+=1\n",
    "            if(pred[p][i]>=threshold and y_test[p][i]==0):\n",
    "                FP+=1\n",
    "    if TP+FP==0:\n",
    "        return 0\n",
    "    return TP/(TP+FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric 2 : using partial correctness\n",
    "\n",
    "def precision_score_weighted(y_test,pred_b):\n",
    "    y_true = np.array(y_test)\n",
    "    y_pred = np.array(pred_b)\n",
    "    pr_list=[]\n",
    "    tmp_p=0\n",
    "    for i in range(y_true.shape[0]):\n",
    "        #print(i)\n",
    "        set_true = set( np.where(y_true[i])[0] )\n",
    "        set_pred = set( np.where(y_pred[i])[0] )\n",
    "        tmp_a = None\n",
    "        if len(set_true) == 0 and len(set_pred) == 0:\n",
    "            tmp_p = 1\n",
    "        elif len(set_pred)==0:\n",
    "            tmp_p=0\n",
    "        else:\n",
    "            tmp_p = len(set_true.intersection(set_pred))/float(len((set_pred)))\n",
    "        pr_list.append(tmp_p)\n",
    "    return np.mean(pr_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall : Metric 1 and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric 1 : using confusion matrix\n",
    "\n",
    "def recall_score_ones_zeros(y_test, pred):\n",
    "    value = 0\n",
    "    TP=0\n",
    "    FN=0\n",
    "    for p in range(0,len(pred)):\n",
    "        for i in range(0,len(pred[p])):\n",
    "            if(pred[p][i]>=threshold and y_test[p][i]==1):\n",
    "                TP+=1\n",
    "            if(pred[p][i]<threshold and y_test[p][i]==1):\n",
    "                FN+=1\n",
    "        \n",
    "    return TP/(TP+FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric 2 : using partial correctness\n",
    "\n",
    "def recall_score_weighted(y_test,pred_b):\n",
    "    y_true = np.array(y_test)\n",
    "    y_pred = np.array(pred_b)\n",
    "    rc_list=[]\n",
    "    for i in range(y_true.shape[0]):\n",
    "        #print(i)\n",
    "        set_true = set( np.where(y_true[i])[0] )\n",
    "        set_pred = set( np.where(y_pred[i])[0] )\n",
    "        tmp_a = None\n",
    "        if len(set_true) == 0 and len(set_pred) == 0:\n",
    "            tmp_r = 1\n",
    "        else:\n",
    "            tmp_r = len(set_true.intersection(set_pred))/float( len((set_true)))\n",
    "        rc_list.append(tmp_r)\n",
    "    return np.mean(rc_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 score : Metric 1 and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric 1 : using confusion matrix\n",
    "\n",
    "def f1_score_ones_zeros(precision,recall):\n",
    "    if precision+recall==0:\n",
    "        return 0\n",
    "    f1 = (2*precision*recall)/float(precision+recall)\n",
    "    return (f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric 2 : using partial correctness\n",
    "\n",
    "def f1_score_weighted(precision,recall):\n",
    "    if precision+recall==0:\n",
    "        return 0\n",
    "    f1 = (2*precision*recall)/float(precision+recall)\n",
    "    return (f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models for Approach 1, 2 and 3\n",
    "\n",
    "load the dataset and label pickle files for corresponding approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 1 : Normal Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pickle files\n",
    "\n",
    "dbfile1=open('datasetFile','rb')\n",
    "dbfile2=open('yFile','rb')\n",
    "db_y=pickle.load(dbfile2)\n",
    "db_data=pickle.load(dbfile1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the testing data\n",
    "\n",
    "db_data_test = db_data[8000:]\n",
    "y_test = db_y[8000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['modelReport/model_100_10.model',\n",
       " 'modelReport/model_75_10.model',\n",
       " 'modelReport/model_50_10.model',\n",
       " 'modelReport/model_100_20.model',\n",
       " 'modelReport/model_75_20.model',\n",
       " 'modelReport/model_50_20.model']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take path for all models for approach 1\n",
    "\n",
    "model_glob = glob.glob(model_original_path + \"*.model\")\n",
    "model_glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For model :  modelReport/model_100_10.model\n",
      "------------NORMAL-----------\n",
      "accuracy :  0.7598671096345515\n",
      "precision :  0.4032485875706215\n",
      "recall :  0.17095808383233532\n",
      "f1 score :  0.2401177460050463\n",
      "-----------WEIGHTED-----------\n",
      "accuracy :  0.18743078626799556\n",
      "precision :  0.38504983388704317\n",
      "recall :  0.1874861572535991\n",
      "f1 score :  0.25218157399255864\n",
      "---------------Percent correct----------------\n",
      "percentage correct :  0.2688622754491018\n",
      "\n",
      "For model :  modelReport/model_75_10.model\n",
      "------------NORMAL-----------\n",
      "accuracy :  0.7780730897009966\n",
      "precision :  0\n",
      "recall :  0.0\n",
      "f1 score :  0\n",
      "-----------WEIGHTED-----------\n",
      "accuracy :  0.0\n",
      "precision :  0.0\n",
      "recall :  0.0\n",
      "f1 score :  0\n",
      "---------------Percent correct----------------\n",
      "percentage correct :  0.15449101796407186\n",
      "\n",
      "For model :  modelReport/model_50_10.model\n",
      "------------NORMAL-----------\n",
      "accuracy :  0.6831229235880398\n",
      "precision :  0.22466281310211947\n",
      "recall :  0.17455089820359282\n",
      "f1 score :  0.19646166807076665\n",
      "-----------WEIGHTED-----------\n",
      "accuracy :  0.2195992195327744\n",
      "precision :  0.23372145757527815\n",
      "recall :  0.7200442967884827\n",
      "f1 score :  0.3528954605346116\n",
      "---------------Percent correct----------------\n",
      "percentage correct :  0.24161676646706587\n",
      "\n",
      "For model :  modelReport/model_100_20.model\n",
      "------------NORMAL-----------\n",
      "accuracy :  0.7635880398671097\n",
      "precision :  0.4120967741935484\n",
      "recall :  0.1529940119760479\n",
      "f1 score :  0.22314410480349342\n",
      "-----------WEIGHTED-----------\n",
      "accuracy :  0.2108387385962137\n",
      "precision :  0.3887889574434425\n",
      "recall :  0.27076411960132885\n",
      "f1 score :  0.31921646168211676\n",
      "---------------Percent correct----------------\n",
      "percentage correct :  0.2877245508982036\n",
      "\n",
      "For model :  modelReport/model_75_20.model\n",
      "------------NORMAL-----------\n",
      "accuracy :  0.7780730897009966\n",
      "precision :  0\n",
      "recall :  0.0\n",
      "f1 score :  0\n",
      "-----------WEIGHTED-----------\n",
      "accuracy :  0.0\n",
      "precision :  0.0\n",
      "recall :  0.0\n",
      "f1 score :  0\n",
      "---------------Percent correct----------------\n",
      "percentage correct :  0.15449101796407186\n",
      "\n",
      "For model :  modelReport/model_50_20.model\n",
      "------------NORMAL-----------\n",
      "accuracy :  0.7780730897009966\n",
      "precision :  0\n",
      "recall :  0.0\n",
      "f1 score :  0\n",
      "-----------WEIGHTED-----------\n",
      "accuracy :  0.0\n",
      "precision :  0.0\n",
      "recall :  0.0\n",
      "f1 score :  0\n",
      "---------------Percent correct----------------\n",
      "percentage correct :  0.15449101796407186\n"
     ]
    }
   ],
   "source": [
    "# run predictions using all 3 metrics on Approach 1 models\n",
    "\n",
    "pred_all = []\n",
    "for model_path in model_glob:\n",
    "    model = load_model(model_path)\n",
    "    pred = model.predict(np.array(db_data_test))\n",
    "    pred_all.append(pred)\n",
    "    print()\n",
    "    print('For model : ',model_path)\n",
    "    print('------------NORMAL-----------')\n",
    "    print('accuracy : ',accuracy_score_ones_zeros(y_test,pred) )\n",
    "    precision = precision_score_ones_zeros(y_test,pred) \n",
    "    print('precision : ',precision)\n",
    "    recall = recall_score_ones_zeros(y_test,pred)\n",
    "    print('recall : ', recall )\n",
    "    print('f1 score : ',f1_score_ones_zeros(precision,recall))\n",
    "    \n",
    "    print('-----------WEIGHTED-----------')\n",
    "    print('accuracy : ',accuracy_score_weighted(y_test,pred) )\n",
    "    precision = precision_score_weighted(y_test,pred) \n",
    "    print('precision : ',precision)\n",
    "    recall = recall_score_weighted(y_test,pred)\n",
    "    print('recall : ', recall )\n",
    "    print('f1 score : ',f1_score_weighted(precision,recall))\n",
    "    \n",
    "    print('---------------Percent correct----------------')\n",
    "    print('percentage correct : ',correct_predictions_top(y_test,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 2 : Center Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pickle files\n",
    "\n",
    "dbfile1_center=open('datasetFile_center','rb')\n",
    "dbfile2_center=open('yFile_center','rb')\n",
    "db_y_center=pickle.load(dbfile2_center)\n",
    "db_data_center=pickle.load(dbfile1_center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the testing data\n",
    "\n",
    "db_data_test = db_data_center[8000:]\n",
    "y_test = db_y_center[8000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['modelReport_center/model_100_10.model',\n",
       " 'modelReport_center/model_75_10.model',\n",
       " 'modelReport_center/model_50_10.model',\n",
       " 'modelReport_center/model_100_20.model',\n",
       " 'modelReport_center/model_75_20.model',\n",
       " 'modelReport_center/model_50_20.model']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take path for all models for approach 2\n",
    "\n",
    "model_glob = glob.glob(model_center_path + \"*.model\")\n",
    "model_glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For model :  modelReport_center/model_100_10.model\n",
      "------------NORMAL-----------\n",
      "accuracy :  0.6694352159468439\n",
      "precision :  0.22831505483549352\n",
      "recall :  0.205688622754491\n",
      "f1 score :  0.21641203339108522\n",
      "-----------WEIGHTED-----------\n",
      "accuracy :  0.23016136687233035\n",
      "precision :  0.23790223065970575\n",
      "recall :  0.8600221483942415\n",
      "f1 score :  0.3727054274831606\n",
      "---------------Percent correct----------------\n",
      "percentage correct :  0.25419161676646707\n",
      "\n",
      "For model :  modelReport_center/model_75_10.model\n",
      "------------NORMAL-----------\n",
      "accuracy :  0.6257807308970099\n",
      "precision :  0.20509521358723623\n",
      "recall :  0.23862275449101797\n",
      "f1 score :  0.22059230556324383\n",
      "-----------WEIGHTED-----------\n",
      "accuracy :  0.22690739861836207\n",
      "precision :  0.22849760059062385\n",
      "recall :  0.9620155038759689\n",
      "f1 score :  0.36928318309461705\n",
      "---------------Percent correct----------------\n",
      "percentage correct :  0.22544910179640718\n",
      "\n",
      "For model :  modelReport_center/model_50_10.model\n",
      "------------NORMAL-----------\n",
      "accuracy :  0.6409302325581395\n",
      "precision :  0.16119500984898227\n",
      "recall :  0.1470059880239521\n",
      "f1 score :  0.1537738803632947\n",
      "-----------WEIGHTED-----------\n",
      "accuracy :  0.1454706533776301\n",
      "precision :  0.18006644518272424\n",
      "recall :  0.2414174972314507\n",
      "f1 score :  0.20627685260028314\n",
      "---------------Percent correct----------------\n",
      "percentage correct :  0.18892215568862275\n",
      "\n",
      "For model :  modelReport_center/model_100_20.model\n",
      "------------NORMAL-----------\n",
      "accuracy :  0.7312956810631229\n",
      "precision :  0.3383838383838384\n",
      "recall :  0.22065868263473054\n",
      "f1 score :  0.26712577020659656\n",
      "-----------WEIGHTED-----------\n",
      "accuracy :  0.22258028792912515\n",
      "precision :  0.22274640088593578\n",
      "recall :  0.9962347729789591\n",
      "f1 score :  0.36408718178132504\n",
      "---------------Percent correct----------------\n",
      "percentage correct :  0.33562874251497005\n",
      "\n",
      "For model :  modelReport_center/model_75_20.model\n",
      "------------NORMAL-----------\n",
      "accuracy :  0.7405980066445182\n",
      "precision :  0.34657236126224156\n",
      "recall :  0.19071856287425148\n",
      "f1 score :  0.24604094244882194\n",
      "-----------WEIGHTED-----------\n",
      "accuracy :  0.21744317882191638\n",
      "precision :  0.22113932394663294\n",
      "recall :  0.9595791805094129\n",
      "f1 score :  0.35944332277383245\n",
      "---------------Percent correct----------------\n",
      "percentage correct :  0.33562874251497005\n",
      "\n",
      "For model :  modelReport_center/model_50_20.model\n",
      "------------NORMAL-----------\n",
      "accuracy :  0.7030564784053156\n",
      "precision :  0.12491694352159469\n",
      "recall :  0.0562874251497006\n",
      "f1 score :  0.07760577915376676\n",
      "-----------WEIGHTED-----------\n",
      "accuracy :  0.10388704318936874\n",
      "precision :  0.15968992248062014\n",
      "recall :  0.13776301218161682\n",
      "f1 score :  0.14791829007140092\n",
      "---------------Percent correct----------------\n",
      "percentage correct :  0.17395209580838322\n"
     ]
    }
   ],
   "source": [
    "# run predictions using all 3 metrics on Approach 2 models\n",
    "\n",
    "pred_center_all = []\n",
    "for model_path in model_glob:\n",
    "    model = load_model(model_path)\n",
    "    pred = model.predict(np.array(db_data_test))\n",
    "    pred_center_all.append(pred)\n",
    "    print()\n",
    "    print('For model : ',model_path)\n",
    "    print('------------NORMAL-----------')\n",
    "    print('accuracy : ',accuracy_score_ones_zeros(y_test,pred) )\n",
    "    precision = precision_score_ones_zeros(y_test,pred) \n",
    "    print('precision : ',precision)\n",
    "    recall = recall_score_ones_zeros(y_test,pred)\n",
    "    print('recall : ', recall )\n",
    "    print('f1 score : ',f1_score_ones_zeros(precision,recall))\n",
    "    \n",
    "    print('-----------WEIGHTED-----------')\n",
    "    print('accuracy : ',accuracy_score_weighted(y_test,pred) )\n",
    "    precision = precision_score_weighted(y_test,pred) \n",
    "    print('precision : ',precision)\n",
    "    recall = recall_score_weighted(y_test,pred)\n",
    "    print('recall : ', recall )\n",
    "    print('f1 score : ',f1_score_weighted(precision,recall))\n",
    "    \n",
    "    print('---------------Percent correct----------------')\n",
    "    print('percentage correct : ',correct_predictions_top(y_test,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 3 : Object Detection Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pickle files\n",
    "\n",
    "dbfile1_obj=open('datasetFile_object','rb')\n",
    "dbfile2_obj=open('yFile_object','rb')\n",
    "db_y_object=pickle.load(dbfile2_obj)\n",
    "db_data_object=pickle.load(dbfile1_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the testing data\n",
    "\n",
    "db_data_test = db_data_object[8000:]\n",
    "y_test = db_y_object[8000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['modelReport_object/model_100_10.model',\n",
       " 'modelReport_object/model_75_10.model',\n",
       " 'modelReport_object/model_50_10.model',\n",
       " 'modelReport_object/model_25_20.model',\n",
       " 'modelReport_object/model_100_20.model',\n",
       " 'modelReport_object/model_75_20.model',\n",
       " 'modelReport_object/model_50_20.model',\n",
       " 'modelReport_object/model_25_10.model']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take path for all models for approach 3\n",
    "\n",
    "model_glob = glob.glob(model_object_path + \"*.model\")\n",
    "model_glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For model :  modelReport_object/model_100_10.model\n",
      "------------NORMAL-----------\n",
      "accuracy :  0.7417275747508305\n",
      "precision :  0.36010230179028135\n",
      "recall :  0.21077844311377245\n",
      "f1 score :  0.26591123701605285\n",
      "-----------WEIGHTED-----------\n",
      "accuracy :  0.22170331698570903\n",
      "precision :  0.22171069978378946\n",
      "recall :  0.9983388704318937\n",
      "f1 score :  0.3628416664180024\n",
      "---------------Percent correct----------------\n",
      "percentage correct :  0.34221556886227544\n",
      "\n",
      "For model :  modelReport_object/model_75_10.model\n",
      "------------NORMAL-----------\n",
      "accuracy :  0.5971428571428572\n",
      "precision :  0.24081477251094613\n",
      "recall :  0.3787425149700599\n",
      "f1 score :  0.29442569533341095\n",
      "-----------WEIGHTED-----------\n",
      "accuracy :  0.2100174023097611\n",
      "precision :  0.2507986605494911\n",
      "recall :  0.5054263565891473\n",
      "f1 score :  0.3352448024493977\n",
      "---------------Percent correct----------------\n",
      "percentage correct :  0.205688622754491\n",
      "\n",
      "For model :  modelReport_object/model_50_10.model\n",
      "------------NORMAL-----------\n",
      "accuracy :  0.7780730897009966\n",
      "precision :  0\n",
      "recall :  0.0\n",
      "f1 score :  0\n",
      "-----------WEIGHTED-----------\n",
      "accuracy :  0.0\n",
      "precision :  0.0\n",
      "recall :  0.0\n",
      "f1 score :  0\n",
      "---------------Percent correct----------------\n",
      "percentage correct :  0.15449101796407186\n",
      "\n",
      "For model :  modelReport_object/model_25_20.model\n",
      "------------NORMAL-----------\n",
      "accuracy :  0.672358803986711\n",
      "precision :  0.17780477926285945\n",
      "recall :  0.131437125748503\n",
      "f1 score :  0.15114477534859702\n",
      "-----------WEIGHTED-----------\n",
      "accuracy :  0.22081210778885196\n",
      "precision :  0.22833570637557352\n",
      "recall :  0.8398671096345515\n",
      "f1 score :  0.3590547541454878\n",
      "---------------Percent correct----------------\n",
      "percentage correct :  0.2284431137724551\n",
      "\n",
      "For model :  modelReport_object/model_100_20.model\n",
      "------------NORMAL-----------\n",
      "accuracy :  0.7441196013289036\n",
      "precision :  0.36957631444614597\n",
      "recall :  0.21676646706586827\n",
      "f1 score :  0.2732591054916022\n",
      "-----------WEIGHTED-----------\n",
      "accuracy :  0.22217238833517902\n",
      "precision :  0.22223145071982284\n",
      "recall :  0.9973421926910299\n",
      "f1 score :  0.3634726013362193\n",
      "---------------Percent correct----------------\n",
      "percentage correct :  0.3461077844311377\n",
      "\n",
      "For model :  modelReport_object/model_75_20.model\n",
      "------------NORMAL-----------\n",
      "accuracy :  0.6681727574750831\n",
      "precision :  0.28935303107488536\n",
      "recall :  0.3401197604790419\n",
      "f1 score :  0.3126892375447288\n",
      "-----------WEIGHTED-----------\n",
      "accuracy :  0.22433238411643727\n",
      "precision :  0.23426435690555295\n",
      "recall :  0.8483942414174972\n",
      "f1 score :  0.36714903788856373\n",
      "---------------Percent correct----------------\n",
      "percentage correct :  0.31167664670658685\n",
      "\n",
      "For model :  modelReport_object/model_50_20.model\n",
      "------------NORMAL-----------\n",
      "accuracy :  0.7768106312292359\n",
      "precision :  0.047619047619047616\n",
      "recall :  0.0002994011976047904\n",
      "f1 score :  0.0005950609937518596\n",
      "-----------WEIGHTED-----------\n",
      "accuracy :  0.000664451827242525\n",
      "precision :  0.0019933554817275745\n",
      "recall :  0.000664451827242525\n",
      "f1 score :  0.0009966777408637875\n",
      "---------------Percent correct----------------\n",
      "percentage correct :  0.15538922155688623\n",
      "\n",
      "For model :  modelReport_object/model_25_10.model\n",
      "------------NORMAL-----------\n",
      "accuracy :  0.5027906976744186\n",
      "precision :  0.22471760797342194\n",
      "recall :  0.5062874251497006\n",
      "f1 score :  0.3112747353888633\n",
      "-----------WEIGHTED-----------\n",
      "accuracy :  0.20481332067710803\n",
      "precision :  0.22403100775193796\n",
      "recall :  0.6150609080841639\n",
      "f1 score :  0.32843294629913244\n",
      "---------------Percent correct----------------\n",
      "percentage correct :  0.15449101796407186\n"
     ]
    }
   ],
   "source": [
    "# run predictions using all 3 metrics on Approach 3 models\n",
    "\n",
    "pred_object_all = []\n",
    "for model_path in model_glob:\n",
    "    model = load_model(model_path)\n",
    "    pred = model.predict(np.array(db_data_test))\n",
    "    pred_object_all.append(pred)\n",
    "    print()\n",
    "    print('For model : ',model_path)\n",
    "    print('------------NORMAL-----------')\n",
    "    print('accuracy : ',accuracy_score_ones_zeros(y_test,pred) )\n",
    "    precision = precision_score_ones_zeros(y_test,pred) \n",
    "    print('precision : ',precision)\n",
    "    recall = recall_score_ones_zeros(y_test,pred)\n",
    "    print('recall : ', recall )\n",
    "    print('f1 score : ',f1_score_ones_zeros(precision,recall))\n",
    "    \n",
    "    print('-----------WEIGHTED-----------')\n",
    "    print('accuracy : ',accuracy_score_weighted(y_test,pred) )\n",
    "    precision = precision_score_weighted(y_test,pred) \n",
    "    print('precision : ',precision)\n",
    "    recall = recall_score_weighted(y_test,pred)\n",
    "    print('recall : ', recall )\n",
    "    print('f1 score : ',f1_score_weighted(precision,recall))\n",
    "    \n",
    "    print('---------------Percent correct----------------')\n",
    "    print('percentage correct : ',correct_predictions_top(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
